{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af662d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1c8b2484d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c73221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment (with also a random version)\n",
    "SIZE = 3\n",
    "\n",
    "\n",
    "class GOPS:\n",
    "    def __init__(self):\n",
    "        self.size = SIZE\n",
    "        self.state, self.score = np.zeros((1, self.size * 3 + 1)), 0.0  # Placeholders\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones((1, self.size * 3 + 1))\n",
    "        self.score = 0.0\n",
    "        self.draw()\n",
    "        return\n",
    "\n",
    "    def draw(self):\n",
    "        remaining = np.nonzero(self.state[0, 1:self.size + 1])[0]\n",
    "        idx = np.random.choice(remaining, 1) + 1\n",
    "\n",
    "        # Update the state\n",
    "        self.state[0, 0] = idx - 1\n",
    "        self.state[0, idx] = 0  # Note conventions with the index\n",
    "        return\n",
    "\n",
    "    def get_illegal_actions(self):\n",
    "        illegal_actions = np.where(self.state[0, self.size + 1:self.size * 2 + 1] == 0)[0]\n",
    "        return illegal_actions\n",
    "\n",
    "    def step(self, action, action_opp):\n",
    "        self.state[0, action + self.size + 1] = 0\n",
    "        self.state[0, action_opp + self.size * 2 + 1] = 0\n",
    "\n",
    "        # Update score\n",
    "        if action > action_opp:\n",
    "            self.score += self.state[0, 0]\n",
    "        elif action < action_opp:\n",
    "            self.score -= self.state[0, 0]\n",
    "\n",
    "        # Game end conditions\n",
    "        done = False\n",
    "        if np.sum(self.state[0, 1:self.size + 1]) == 0:\n",
    "            if self.score > 0:\n",
    "                self.score += SIZE**2  # Best score for one player is nChoose2 so use n^2 to denote winning\n",
    "            elif self.score < 0:\n",
    "                self.score -= SIZE**2\n",
    "            done = True\n",
    "            self.state[0, 0] = -1  # This signifies no current value card. Probably unnecessary\n",
    "        else:\n",
    "            self.draw()\n",
    "        return self.state.copy(), copy(self.score), done  # These copies are necessary I think\n",
    "\n",
    "\n",
    "\n",
    "class RandomGOPS:\n",
    "    def __init__(self):\n",
    "        self.size = SIZE\n",
    "        self.state, self.score = np.zeros((1, self.size * 3 + 1)), 0.0  # Placeholders\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones((1, self.size * 3 + 1))\n",
    "        self.score = 0.0\n",
    "        self.draw()\n",
    "        return\n",
    "\n",
    "    def draw(self):\n",
    "        remaining = np.nonzero(self.state[0, 1:self.size + 1])[0]\n",
    "        idx = np.random.choice(remaining, 1) + 1\n",
    "\n",
    "        # Update the state\n",
    "        self.state[0, 0] = idx - 1\n",
    "        self.state[0, idx] = 0  # Note conventions with the index\n",
    "        return\n",
    "\n",
    "    def get_illegal_actions(self):\n",
    "        illegal_actions = np.where(self.state[0, self.size + 1:self.size * 2 + 1] == 0)[0]\n",
    "        #         legal_actions_mask = np.zeros((SIZE))\n",
    "        #         legal_actions_mask[legal_actions] = 1\n",
    "        return illegal_actions\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update game with random opponent move\n",
    "        action_opp = np.random.choice(np.nonzero(self.state[0, self.size * 2 + 1:])[0])\n",
    "        self.state[0, action + self.size + 1] = 0\n",
    "        self.state[0, action_opp + self.size * 2 + 1] = 0\n",
    "\n",
    "        # Update score\n",
    "        if action > action_opp:\n",
    "            self.score += self.state[0, 0]\n",
    "        elif action < action_opp:\n",
    "            self.score -= self.state[0, 0]\n",
    "\n",
    "        # Game end conditions\n",
    "        done = False\n",
    "        if np.sum(self.state[0, 1:self.size + 1]) == 0:\n",
    "            if self.score > 0:\n",
    "                self.score += SIZE**2  # Best score for one player is nChoose2 so use n^2 to denote winning\n",
    "            elif self.score < 0:\n",
    "                self.score -= SIZE**2\n",
    "            done = True\n",
    "            self.state[0, 0] = -1  # This signifies no current value card. Probably unnecessary\n",
    "        else:\n",
    "            self.draw()\n",
    "        return self.state.copy(), copy(self.score), done  # These copies are necessary I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb2cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN representing s->p(a) map\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, widths=[8, 8], path=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = SIZE * 3 + 1  # Player hands and value cards, and the current card and score\n",
    "        self.num_actions = SIZE\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.num_features, widths[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        for i in range(len(widths) - 1):\n",
    "            self.layers.append(nn.Linear(widths[i], widths[i + 1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(widths[-1], self.num_actions))\n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        if path is not None:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "        return\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)  # Environment is numpy-based; convert\n",
    "        action_probs = self.layers(state)\n",
    "        return action_probs\n",
    "\n",
    "    def get_action(self, state, illegal_actions):\n",
    "        # Use legal actions to mask\n",
    "        action_probs = self.forward(state)\n",
    "        action_probs[0, illegal_actions] = 0\n",
    "\n",
    "        cat = Categorical(probs=action_probs)  # Constructs multinomial from the probs\n",
    "        action = cat.sample()\n",
    "        return action.item()\n",
    "    \n",
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b88bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting batches\n",
    "# The reason we are temporarily storing in a deque is for future compatibility reasons\n",
    "def get_batch(env, memory, num_batches, policy):\n",
    "    for batch in range(num_batches):\n",
    "        done = False\n",
    "        state = env.state.copy()\n",
    "        while not done:\n",
    "            action = policy.get_action(state, env.get_illegal_actions())\n",
    "            state_new, reward, done = env.step(action)\n",
    "\n",
    "            memory.append([state, action, reward, done])\n",
    "\n",
    "            state = state_new\n",
    "        env.reset()\n",
    "    return\n",
    "\n",
    "# Processing batches into stacks\n",
    "def stack_batch(memory):\n",
    "    states = np.concatenate([s for (s, a, lp, r, done) in memory])\n",
    "    actions = np.stack([a for (s, a, lp, r, done) in memory])\n",
    "    rewards = np.stack([r for (s, a, lp, r, done) in memory])\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbd784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, model, count):\n",
    "    rewards = []\n",
    "    for t in range(count):\n",
    "        state = env.state.copy()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = model.get_action(state,env.get_illegal_actions())\n",
    "            state_new, reward, done = env.step(action)\n",
    "            state = state_new\n",
    "        env.reset()\n",
    "        rewards.append(reward)\n",
    "    return sum(rewards)/(100*count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331372d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "1 -6.556675913316212e-08\n",
      "2 1.1195545084774494e-05\n",
      "3 0.0\n",
      "4 4.803005140274763e-07\n",
      "5 1.2064585462212563e-05\n",
      "6 1.8015271052718163e-07\n",
      "7 2.0631705410778522e-07\n",
      "8 9.19075682759285e-06\n",
      "9 1.267326297238469e-07\n",
      "10 2.0397419575601816e-07\n",
      "11 1.2220581993460655e-05\n",
      "12 -2.519422359625878e-08\n",
      "13 9.219365892931819e-08\n",
      "14 1.1462834663689137e-05\n",
      "15 5.812034942209721e-07\n",
      "16 3.1324452720582485e-07\n",
      "17 1.2984848581254482e-05\n",
      "18 0.0\n",
      "19 1.0387157090008259e-07\n",
      "20 1.2148404493927956e-05\n",
      "21 -6.212965075746979e-08\n",
      "22 5.920883268117905e-07\n",
      "23 9.164214134216309e-06\n",
      "24 3.9217411540448666e-07\n",
      "25 4.2532337829470634e-07\n",
      "26 9.158742614090443e-06\n",
      "27 -9.264986999824032e-08\n",
      "28 1.5895057003945112e-07\n",
      "29 1.3704528100788593e-05\n",
      "30 3.8222060538828373e-07\n",
      "31 4.4517219066619873e-07\n",
      "32 1.3679498806595802e-05\n",
      "33 1.1920928955078125e-07\n",
      "34 5.094625521451235e-07\n",
      "35 1.115433406084776e-05\n",
      "36 -5.62691084837752e-08\n",
      "37 0.0\n",
      "38 -1.1920928955078125e-07\n",
      "39 0.0\n",
      "40 1.5451223589479923e-07\n",
      "41 1.1290307156741619e-05\n",
      "42 1.2782402336597443e-07\n",
      "43 1.070613507181406e-06\n",
      "44 1.3303826563060284e-05\n",
      "45 5.844340194016695e-07\n",
      "46 3.4880940802395344e-07\n",
      "47 1.3639568351209164e-05\n",
      "48 8.719507604837418e-08\n",
      "49 1.1834345059469342e-07\n",
      "50 1.1367839761078358e-05\n",
      "51 5.471520125865936e-08\n",
      "52 2.1376035874709487e-07\n",
      "53 1.0702526196837425e-05\n",
      "54 -2.6559870747178138e-08\n",
      "55 -9.56268166873997e-08\n",
      "56 5.394294078087114e-08\n",
      "57 3.857421688735485e-07\n",
      "58 4.417670425027609e-07\n",
      "59 9.221024811267853e-06\n",
      "60 4.469475243240595e-07\n",
      "61 4.1152816265821457e-07\n",
      "62 9.415554814040661e-06\n",
      "63 0.0\n",
      "64 9.28884631434812e-08\n",
      "65 9.333282235957086e-08\n",
      "66 0.0\n",
      "67 1.467997208237648e-07\n",
      "68 1.1934665963053703e-05\n",
      "69 2.965825842693448e-07\n",
      "70 5.788169801235199e-07\n",
      "71 1.3688579201698303e-05\n",
      "72 0.0\n",
      "73 5.271867848932743e-07\n",
      "74 1.2031756341457367e-05\n",
      "75 -5.707732242399288e-08\n",
      "76 0.0\n",
      "77 9.400071576237679e-06\n",
      "78 -1.1920928244535389e-07\n",
      "79 0.0\n",
      "80 -9.587924409970583e-08\n",
      "81 5.002220859751105e-08\n",
      "82 1.9279832486063242e-07\n",
      "83 1.0759104043245316e-05\n",
      "84 4.223111318424344e-07\n",
      "85 7.427297532558441e-08\n",
      "86 1.0745017789304256e-05\n",
      "87 1.7764978110790253e-07\n",
      "88 1.3686076272279024e-07\n",
      "89 9.1862166300416e-06\n",
      "90 5.660112947225571e-07\n",
      "91 1.1608062777668238e-07\n",
      "92 9.305309504270554e-06\n",
      "93 5.902547854930162e-07\n",
      "94 1.023843651637435e-06\n",
      "95 1.3128388673067093e-05\n",
      "96 6.29421847975209e-08\n",
      "97 1.1920928955078125e-07\n",
      "98 1.1596130207180977e-05\n",
      "99 0.0\n"
     ]
    }
   ],
   "source": [
    "E = RandomGOPS()\n",
    "M = PolicyNet()\n",
    "Mem = deque(maxlen=10000)\n",
    "optim = torch.optim.SGD(M.parameters(), lr=0.01)\n",
    "loss_fn = nn.KLDivLoss(log_target=True, reduction='sum')\n",
    "normaliser = nn.LogSoftmax(dim=1)\n",
    "\n",
    "get_batch(E, Mem, 1000, M)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for i in range(100):\n",
    "    s, a, r, _ = Mem[i]\n",
    "    logprobs = torch.log(M(s))    \n",
    "    tweaked_logprobs = torch.clone(logprobs).detach()\n",
    "    tweaked_logprobs[0, a] -= 0.001*r # Tweak policy\n",
    "    norm_tweaked_logprobs = normaliser(tweaked_logprobs)\n",
    "        \n",
    "    loss = loss_fn(logprobs, norm_tweaked_logprobs)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    print(i, loss.item())\n",
    "    train_losses.append(loss.item())\n",
    "    test_losses.append(test(E, M, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b376b64c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_losses\u001b[49m)), train_losses)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(range(len(train_losses)), train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1425b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(test_losses)), test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c36e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc4cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
